{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 10:19:04.250418: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# env py_dl\n",
    "import os\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# from __future__ import print_function, division\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D, regularization\n",
    "from keras.layers import Embedding, Reshape, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot \n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import initializers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "%matplotlib inline\n",
    "\n",
    "# nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "L_RBNS = 20 # length of each sequence in RBNS data\n",
    "O = int(1e7) # for initializing big arrays, helps reduce runtime\n",
    "LIMIT_FILE_N_SEQ_READ = int(1e6) # limit the amount of seq we read from file, helps reduce runtime\n",
    "ONE_HOT_DICT = {b'A': np.array([0,0,0,1]),\n",
    "                b'C': np.array([0,0,1,0]),\n",
    "                b'G': np.array([0,1,0,0]),\n",
    "                b'T': np.array([1,0,0,0]),\n",
    "                b'U': np.array([1,0,0,0]),\n",
    "                b'N': np.array([0.25,0.25,0.25,0.25])\n",
    "                }\n",
    "\n",
    "ONE_HOT_DICT2 = {'A':[0,0,0,1],\n",
    "                'C': [0,0,1,0],\n",
    "                'G':[0,1,0,0],\n",
    "                'T': [1,0,0,0],\n",
    "                'U': [1,0,0,0],\n",
    "                'N': [0.25,0.25,0.25,0.25]\n",
    "                }\n",
    "\n",
    "L_RBNS = 20 # length of each sequence in RBNS data\n",
    "FILES_40 = [\"RBP9\" , \"RBP11\", \"RBP15\", \"RBP31\"] # files  with sequences len 40 and not 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RBP10_1300nM.seq', 'RBP10_20nM.seq', 'RBP10_320nM.seq', 'RBP10_5nM.seq', 'RBP10_80nM.seq', 'RBP10_input.seq', 'RBP11_1090nM.seq', 'RBP11_121nM.seq', 'RBP11_13nM.seq', 'RBP11_1nM.seq', 'RBP1_1300nM.seq', 'RBP11_3280nM.seq', 'RBP11_365nM.seq', 'RBP11_40nM.seq', 'RBP11_4nM.seq', 'RBP11_9800nM.seq', 'RBP11_input.seq', 'RBP1_20nM.seq', 'RBP12_1300nM.seq', 'RBP12_20nM.seq', 'RBP12_320nM.seq', 'RBP12_5nM.seq', 'RBP12_80nM.seq', 'RBP12_input.seq', 'RBP13_1300nM.seq', 'RBP1_320nM.seq', 'RBP13_20nM.seq', 'RBP13_320nM.seq', 'RBP13_5nM.seq', 'RBP13_80nM.seq', 'RBP13_input.seq', 'RBP14_1300nM.seq', 'RBP14_20nM.seq', 'RBP14_320nM.seq', 'RBP14_5nM.seq', 'RBP14_80nM.seq', 'RBP14_input.seq', 'RBP15_1300nM.seq', 'RBP15_20nM.seq', 'RBP15_320nM.seq', 'RBP15_5nM.seq', 'RBP15_80nM.seq', 'RBP15_input.seq', 'RBP1_5nM.seq', 'RBP16_1300nM.seq', 'RBP16_20nM.seq', 'RBP16_320nM.seq', 'RBP16_5nM.seq', 'RBP16_80nM.seq', 'RBP16_input.seq', 'RBP1_80nM.seq', 'RBP1_input.seq', 'RBP2_1300nM.seq', 'RBP2_20nM.seq', 'RBP2_320nM.seq', 'RBP2_5nM.seq', 'RBP2_80nM.seq', 'RBP2_input.seq', 'RBP3_1300nM.seq', 'RBP3_20nM.seq', 'RBP3_320nM.seq', 'RBP3_5nM.seq', 'RBP3_80nM.seq', 'RBP3_input.seq', 'RBP4_1300nM.seq', 'RBP4_20nM.seq', 'RBP4_320nM.seq', 'RBP4_5nM.seq', 'RBP4_80nM.seq', 'RBP4_input.seq', 'RBP5_1300nM.seq', 'RBP5_20nM.seq', 'RBP5_320nM.seq', 'RBP5_5nM.seq', 'RBP5_80nM.seq', 'RBP5_input.seq', 'RBP6_1300nM.seq', 'RBP6_20nM.seq', 'RBP6_320nM.seq', 'RBP6_5nM.seq', 'RBP6_80nM.seq', 'RBP6_input.seq', 'RBP7_1300nM.seq', 'RBP7_20nM.seq', 'RBP7_320nM.seq', 'RBP7_5nM.seq', 'RBP7_80nM.seq', 'RBP7_input.seq', 'RBP8_1300nM.seq', 'RBP8_20nM.seq', 'RBP8_320nM.seq', 'RBP8_5nM.seq', 'RBP8_80nM.seq', 'RBP8_input.seq', 'RBP9_1300nM.seq', 'RBP9_20nM.seq', 'RBP9_320nM.seq', 'RBP9_5nM.seq', 'RBP9_80nM.seq', 'RBP9_input.seq']\n"
     ]
    }
   ],
   "source": [
    "# list of files in RBNS_training\n",
    "directory = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RBNS_training'\n",
    "RBNS_training_files = []\n",
    "for filename in os.listdir(directory): # Iterate over all files in the directory\n",
    "    if os.path.isfile(os.path.join(directory, filename)):\n",
    "        RBNS_training_files.append(filename)\n",
    "print(RBNS_training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RBP1_20nM.seq', 'RBP1_80nM.seq', 'RBP1_input.seq']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out protein 1 files only\n",
    "PROTEIN = \"RBP1\" ### TODO change \n",
    "filtered_list = [value for value in RBNS_training_files if value.startswith(str(PROTEIN)+'_')]\n",
    "\n",
    "prefixes = [item.split('_', 1)[0] for item in filtered_list]\n",
    "suffixes = [item.split('_', 1)[1] for item in filtered_list]\n",
    "\n",
    "# find the middle affinity files\n",
    "modified_list = [value.replace('RBP1_', '').replace('nM.seq', '') for value in filtered_list]\n",
    "if 'RBP1_input.seq' in filtered_list: # get rid of input\n",
    "    modified_list.remove('input.seq')\n",
    "modified_list = sorted(modified_list, key=int)\n",
    "\n",
    "modified_list = [PROTEIN + \"_\" + value + \"nM.seq\" for value in modified_list[1:3]]  # the first 2 files\n",
    "modified_list.append(str(PROTEIN+'_input.seq')) # 0\n",
    "modified_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINARY - initalize np.arrays\n",
    "master_list = np.empty((O), dtype=f'|S{L_RBNS}') # sequences\n",
    "# class_lables = np.zeros((O, len(modified_list)), dtype=np.bool_) # array of probolities per class\n",
    "class_lables = np.zeros((O, len(modified_list))) # array of probolities per class\n",
    "n = 0\n",
    "\n",
    "## running on protein 1 files only ############################################################\n",
    "for file_index, file in enumerate(modified_list):\n",
    "    #print(file_index)\n",
    "    file_path = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RBNS_training/' + file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        # choose sequences from file randomly - limited to LIMIT_FILE_N_SEQ_READ\n",
    "        rng = np.random.default_rng(seed=123)\n",
    "        rand_indices = rng.choice(len(lines), size=LIMIT_FILE_N_SEQ_READ, replace=False)\n",
    "\n",
    "        # cut seq to 20 length randomly if len is longer find a random starting index and take 20 from there\n",
    "        if file in FILES_40: # file len 40\n",
    "            # choose random index to start from and select 20 chars - save shortend seq into master_list\n",
    "            start_index = random.randint(0, 20)\n",
    "            for index in rand_indices:\n",
    "                seq = lines[index].split('\\t')[0] # take only the sequence\n",
    "                master_list[n] = seq[start_index:start_index+20] # shortened seq\n",
    "                class_lables[n, file_index] = 1 # lables - 1 if in file otherwise stays 0\n",
    "                n += 1\n",
    "        if file == str(PROTEIN+'_input.seq'):\n",
    "              for index in rand_indices:\n",
    "                    seq = lines[index].split('\\t')[0] # take only the sequence\n",
    "                    master_list[n] = seq # seq\n",
    "                    class_lables[n, file_index] = 0 # lables - 1 if in file otherwise stays 0\n",
    "                    n += 1\n",
    "        else: # file len 20\n",
    "            for index in rand_indices:\n",
    "                    seq = lines[index].split('\\t')[0] # take only the sequence\n",
    "                    master_list[n] = seq # seq\n",
    "                    class_lables[n, file_index] = 1 # lables - 1 if in file otherwise stays 0\n",
    "                    n += 1\n",
    "\n",
    "# free memory initlized that wasnt used \n",
    "master_list = master_list[:n]\n",
    "class_lables = class_lables[:n]\n",
    "del lines\n",
    "\n",
    "# binary vec\n",
    "class_lables_binary = np.where(((class_lables[:, 0] == 1) | (class_lables[:, 1] == 1)) & (class_lables[:, 2] == 0), 1, 0)\n",
    "class_lables_binary\n",
    "\n",
    "# convert master_list seqences to one hot encoding\n",
    "one_hot = np.zeros((len(master_list),  L_RBNS, 4), dtype=np.float16)\n",
    "one_hot = np.array([ONE_HOT_DICT[bytes([nuc])] for seq in master_list for nuc in seq])\n",
    "one_hot = one_hot.reshape(len(master_list), -1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MULTICALSS = initalize np.arrays\n",
    "# master_list = np.empty((O), dtype=f'|S{L_RBNS}') # sequences\n",
    "# # class_lables = np.zeros((O, len(filtered_list)), dtype=np.bool_) # array of probolities per class\n",
    "# class_lables = np.zeros((O, len(filtered_list))) # array of probolities per class\n",
    "# n = 0\n",
    "\n",
    "# ## running on protein 1 files only ############################################################\n",
    "# for file_index, file in enumerate(filtered_list):\n",
    "#     #print(file_index)\n",
    "#     file_path = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RBNS_training/' + file\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         lines = file.readlines()\n",
    "#         # choose sequences from file randomly - limited to LIMIT_FILE_N_SEQ_READ\n",
    "#         rng = np.random.default_rng(seed=123)\n",
    "#         rand_indices = rng.choice(len(lines), size=LIMIT_FILE_N_SEQ_READ, replace=False)\n",
    "\n",
    "#         # cut seq to 20 length randomly if len is longer find a random starting index and take 20 from there\n",
    "#         if file in FILES_40: # file len 40\n",
    "#             # choose random index to start from and select 20 chars - save shortend seq into master_list\n",
    "#             start_index = random.randint(0, 20)\n",
    "#             for index in rand_indices:\n",
    "#                 seq = lines[index].split('\\t')[0] # take only the sequence\n",
    "#                 master_list[n] = seq[start_index:start_index+20] # shortened seq\n",
    "#                 class_lables[n, file_index] = 1 # lables - 1 if in file otherwise stays 0\n",
    "#                 n += 1\n",
    "#         else: # file len 20\n",
    "#             for index in rand_indices:\n",
    "#                     seq = lines[index].split('\\t')[0] # take only the sequence\n",
    "#                     master_list[n] = seq # seq\n",
    "#                     class_lables[n, file_index] = 1 # lables - 1 if in file otherwise stays 0\n",
    "#                     n += 1\n",
    "# # free memory initlized that wasnt used \n",
    "# master_list = master_list[:n]\n",
    "# class_lables = class_lables[:n]\n",
    "# del lines\n",
    "# # # convert master_list seqences to one hot encoding\n",
    "# one_hot = np.zeros((len(master_list),  L_RBNS, 4), dtype=np.float16)\n",
    "# one_hot = np.array([ONE_HOT_DICT[bytes([nuc])] for seq in master_list for nuc in seq])\n",
    "# one_hot = one_hot.reshape(len(master_list), -1, 4)\n",
    "\n",
    "# # for i, seq in enumerate(master_list): # each seq\n",
    "# #     for j, nuc in enumerate(seq): # each nucleotide\n",
    "# #         one_hot[i, j, :] = ONE_HOT_DICT[bytes([nuc])] # dim 6000000 seq * 20 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut seg from RNAcomplete file into all shifts of len 20\n",
    "RNAcompete_master_list_one_hot = np.empty((O), dtype=f'|S{L_RBNS*2}') # sequences\n",
    "\n",
    "# read txt file\n",
    "file_path = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RNAcompete_sequences.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    # convert seqences to one hot encoding\n",
    "    RNAcompete_master_list_one_hot = [[ONE_HOT_DICT2[nuc] for nuc in seq.rstrip('\\n')] for seq in file]\n",
    "# cut seg from RNAcomplete file into all shifts of len 20\n",
    "shifts_RNAcompete_master_list = np.empty((O), dtype=object) # sequences\n",
    "\n",
    "for n, seq_onehot in enumerate(RNAcompete_master_list_one_hot):\n",
    "    line_length = len(seq_onehot)\n",
    "    if line_length > 20: # create k-mers of all possible shifts \n",
    "        shifts = np.empty((line_length - 20 + 1), dtype=object)\n",
    "        for i in range(line_length - 20 + 1):\n",
    "            shifts[i] = np.array(seq_onehot[i:i+20])\n",
    "\n",
    "        shifts_RNAcompete_master_list[n] = shifts\n",
    "    else:\n",
    "        shifts_RNAcompete_master_list[n] = seq_onehot\n",
    "                \n",
    "# shifts_RNAcompete_master_list[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000000,) (3000000, 3) (3000000, 20, 4)\n",
      "(2400000, 20, 4) (2400000,) (600000, 20, 4) (600000,)\n"
     ]
    }
   ],
   "source": [
    "# split into training and validation sets -- 80% training, 20% validation\n",
    "# TODO LATER: keep out 6~ proteins for validation of the end\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "one_hot_train_X, one_hot_valid_X, one_hot_train_Y, one_hot_valid_Y = train_test_split(one_hot, class_lables_binary, test_size=0.2, random_state=42)\n",
    "# train_X, test_X, train_y, test_y = train_test_split(one_hot, class_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(master_list.shape,class_lables.shape, one_hot.shape)\n",
    "print(one_hot_train_X.shape, one_hot_train_Y.shape, one_hot_valid_X.shape, one_hot_valid_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the layer size is: 32\n"
     ]
    }
   ],
   "source": [
    "# 2) NN per protien - input each seq (len 20) in master_list into NN \n",
    "    # output = vec of probabilities (one per each concentration)\n",
    "    # compare output to bool numpy array (true lable)\n",
    "    # backpropogation to model to minimize loss?\n",
    "\n",
    "model_param_dict = {\"kernel_size\":3, \"pool_size\":2, \"layers\": [32], \"final_activation_function\":\"softmax\"}\n",
    "output_size=1 # number of classes\n",
    "\n",
    "# # first try - simple NN\n",
    "# model_1=Sequential()\n",
    "# model_1.add(Dense(11, activation=\"relu\",input_shape=one_hot_train_X.shape[1:]))\n",
    "# model_1.add(Flatten())\n",
    "# model_1.add(Dense(output_size, activation=model_param_dict['final_activation_function'])) # softmax\n",
    "\n",
    "# second try - CNN\n",
    "model_2 = Sequential()\n",
    "model_2.add(Conv1D(filters=32, kernel_size=model_param_dict[\"kernel_size\"], strides=1,\\\n",
    "                 kernel_initializer=initializers.RandomNormal(stddev=0.01), \\\n",
    "                 activation='relu',\\\n",
    "                 input_shape=one_hot_train_X.shape[1:], use_bias=True, bias_initializer='RandomNormal'))\n",
    "\n",
    "model_2.add(MaxPooling1D(pool_size=model_param_dict[\"pool_size\"], strides=None, padding='valid', data_format='channels_last'))\n",
    "model_2.add(Flatten())\n",
    "# per layer\n",
    "for layer_size in model_param_dict['layers']:\n",
    "            print(f'the layer size is: {layer_size}')\n",
    "            model_2.add(Dense(layer_size, activation='relu'))\n",
    "model_2.add(Dense(output_size, activation=model_param_dict['final_activation_function'])) # softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_4 (Conv1D)           (None, 18, 32)            416       \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 9, 32)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 288)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                9248      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,697\n",
      "Trainable params: 9,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(2400000, 20, 4)\n",
      "(2400000,)\n",
      "(600000, 20, 4)\n",
      "(600000,)\n",
      "Epoch 1/2\n",
      "300000/300000 [==============================] - 1273s 4ms/step - loss: 0.6360 - accuracy: 0.6667 - val_loss: 0.6357 - val_accuracy: 0.6666\n",
      "Epoch 2/2\n",
      "300000/300000 [==============================] - 1560s 5ms/step - loss: 0.6357 - accuracy: 0.6667 - val_loss: 0.6357 - val_accuracy: 0.6666\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "# Hyperparameters\n",
    "EPHOCHS = 1\n",
    "\n",
    "model = model_2\n",
    "model.compile(optimizer=SGD(lr = .003), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# Generate dummy data for training\n",
    "X_train_norm = one_hot_train_X\n",
    "y_train = one_hot_train_Y\n",
    "\n",
    "print(X_train_norm.shape)\n",
    "print(y_train.shape)\n",
    "print(one_hot_valid_X.shape)\n",
    "print(one_hot_valid_Y.shape)\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "run_hist_1 = model.fit(X_train_norm, y_train, batch_size=8, epochs=EPHOCHS, validation_data=(one_hot_valid_X, one_hot_valid_Y), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - hyperparameter tuning\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "# hyperparameters to play with:\n",
    "# 1) number of layers -- 1, 2, 3\n",
    "# 2) number of neurons per layer -- 32, 64, 128\n",
    "# 3) activation function -- \"relu\", \"sigmoid\", \"tanh\"\n",
    "# 4) optimizer -- \"adam\", \"sgd\", \"rmsprop\"\n",
    "# 5) learning rate -- 0.001, 0.01, 0.1\n",
    "# 6) batch size -- 8, 16, 32\n",
    "# 7) number of epochs -- 2, 4, 8\n",
    "# 8) kernel size -- 3, 5, 7\n",
    "# 9) pool size -- 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO check the validation set - might have shape prob?\n",
    "model.predict(one_hot_valid_X)\n",
    "# compare to lables - one_hot_valid_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_valid_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one_hot_valid_X.shape\n",
    "one_hot_valid_X[0]\n",
    "# one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regresstion - maximize\n",
    "# input: features\n",
    "# output:\n",
    "import statsmodels.api as sm\n",
    "# feature data X and target labels y\n",
    "# X should be a 2D array or DataFrame containing your features\n",
    "# y should be a 1D array or Series containing your target labels (binary: 0 or 1)\n",
    "# Add a constant to the feature data to fit the intercept term\n",
    "X_with_constant = sm.add_constant(X)\n",
    "\n",
    "# Create a logistic regression model\n",
    "logit_model = sm.Logit(y, X_with_constant)\n",
    "logit_result = logit_model.fit()\n",
    "print(logit_result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "correlation_coefficient, p_value = stats.pearsonr(x, y)\n",
    "print(\"Pearson correlation coefficient:\", correlation_coefficient, \"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY THIS AS WELL\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# hyperparameters to play with: \n",
    "# 1) number of layers\n",
    "# 2) number of neurons per layer\n",
    "# 3) activation function\n",
    "# 4) optimizer\n",
    "# 5) learning rate\n",
    "# 6) batch size\n",
    "# 7) number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut seg from RNAcomplete file into all shifts of len 20\n",
    "shifts_RNAcompete_master_list = np.empty((O), dtype=object) # sequences\n",
    "\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "        for n, line in enumerate(file):\n",
    "            line = line.rstrip('\\n')  # Remove newline character from the end\n",
    "            line_length = len(line)\n",
    "            if line_length > 20: # create k-mers of all possible shifts \n",
    "                shifts = np.empty((line_length - 20 + 1), dtype=f'|S{L_RBNS}')\n",
    "                for i in range(line_length - 20 + 1):\n",
    "                    shifts[i] = line[i:i+20]\n",
    "                    #shifts_RNAcompete_master_list[n] = line[i:i+20]\n",
    "                    # n += 1\n",
    "                shifts_RNAcompete_master_list[n] = shifts\n",
    "            else:\n",
    "                shifts_RNAcompete_master_list[n] = line\n",
    "                \n",
    "shifts_RNAcompete_master_list[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input to NN shifts_RNAcompete_master_list sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) take all sequences from file RNAcompete_2009_dataset.txt and input into NNcomplete_sequnces.txt (seq len 30-40)\n",
    "# create all possible k-mers with shift 1 (len 20) \n",
    "# input into NN\n",
    "# choose max and min (features) probability for each full sequence - find best k-mer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) model feature to find RNCMPT_train on all proteins\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regresstion -maximize \n",
    "# input: features\n",
    "# output: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
